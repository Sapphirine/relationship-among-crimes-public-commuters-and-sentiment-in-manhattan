{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HTML module\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "# common modules\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# plot modules\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# import Station_loc_management\n",
    "#!pip install ipynb\n",
    "#from ipynb.fs.full.Station_loc_management import *\n",
    "\n",
    "# random seed\n",
    "seed = 2\n",
    "\n",
    "# Set the jupyter to display all the columns adn rows\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "'''\n",
    "import pandas as pd\n",
    "# download the data\n",
    "path = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_180519.txt'\n",
    "col_name_new = ['C/A','UNIT','SCP','STATION','LINENAME','DIVISION','DATE','TIME','DESC','ENTRIES','EXITS']\n",
    "MTA_data_20180519w = pd.read_csv(path, sep=\",\", header=0, names = col_name_new)\n",
    "\n",
    "MTA_data_20180519w.head()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset from MTA webset\n",
    "\n",
    "def download_data(url,start,stop,log_name):\n",
    "    \"\"\"\n",
    "    This function download selected datasets(prior 10/18/14) from MTA website, by finding the selected datasets by HTML elements.\n",
    "    Then consolidate all the downloaded datasets to one large dataframe.\n",
    "    Also, log the downloaded files information, such as file date, file_link, number of rows, and number of columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    col_name = ['C/A','UNIT','SCP','DATE1','TIME1','DESC1','ENTRIES1','EXITS1','DATE2','TIME2','DESC2','ENTRIES2','EXITS2',\n",
    "            'DATE3','TIME3','DESC3','ENTRIES3','EXITS3','DATE4','TIME4','DESC4','ENTRIES4','EXITS4',\n",
    "            'DATE5','TIME5','DESC5','ENTRIES5','EXITS5','DATE6','TIME6','DESC6','ENTRIES6','EXITS6',\n",
    "            'DATE7','TIME7','DESC7','ENTRIES7','EXITS7','DATE8','TIME8','DESC8','ENTRIES8','EXITS8']\n",
    "    df = pd.DataFrame(columns = col_name)\n",
    "\n",
    "    # Log of downloading\n",
    "    logFile  = open(log_name, 'w')  # 'Download_Log.txt'\n",
    "    logFile.write('file_date;file_link;number_of_rows;number_of_columns')\n",
    "    \n",
    "    # get HTML code\n",
    "    jpy = pq(url) \n",
    "\n",
    "    for i in range(start, stop, 2):\n",
    "        # retrieve file path\n",
    "        item = jpy('#contentbox > div > div > a:nth-child({})'.format(i))\n",
    "        fileDate = item.text()\n",
    "        filePath = 'http://web.mta.info/developers/' + item.attr('href')\n",
    "\n",
    "        # get data from txt, save to csv, and append to datafram\n",
    "        data = pd.read_csv(filePath, sep=\",\", header=None, names = col_name)\n",
    "        df = df.append(data, ignore_index=True)\n",
    "        # data.to_csv('./raw_data/MTA_data_{}.csv'.format(re.sub(r',','',fileDate)))\n",
    "\n",
    "        # log the downloaded file information\n",
    "        record = str(fileDate) + ';' + str(filePath) + ';' + str(data.shape[0]) + ';' + str(data.shape[1])\n",
    "        logFile.write('\\n' + record)\n",
    "    \n",
    "    logFile.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wide form dataset to long form dataset\n",
    "def convert_wide_to_long(df):\n",
    "    \"\"\"\n",
    "    In original data, there are multiple data points included in each row. \n",
    "    This function is to convert the wide form DF to long form DF where there is only one data point per row.\n",
    "    Also, set the 'ENTRIES' and 'EXITS' variables type to float.\n",
    "    \"\"\"\n",
    "    \n",
    "    col_name_long = ['C/A','UNIT','SCP','DATE','TIME','DESC','ENTRIES','EXITS']\n",
    "    df_long = pd.DataFrame(columns = col_name_long)\n",
    "\n",
    "    for i in range(0,8):\n",
    "        ind = list(range(0,3)) + [5*i+3, 5*i+4, 5*i+5, 5*i+6, 5*i+7]\n",
    "        temp = df.iloc[:,ind]\n",
    "        temp.columns = col_name_long\n",
    "        df_long = df_long.append(temp, ignore_index = True)\n",
    "        df_long = df_long.sort_values(['C/A','UNIT','SCP','DATE','TIME'])\n",
    "        df_long[['ENTRIES','EXITS']] = df_long[['ENTRIES','EXITS']].apply(pd.to_numeric)\n",
    "        \n",
    "    return df_long.reset_index(drop = True).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irregular_event(df):\n",
    "    \"\"\"\n",
    "    In the original data, there are factors that may impact the data. (ie. Hardware failure, \"IRREGULAR\" audit event)\n",
    "    Clean the data to filter out 'IRREGULAR' audit event.\n",
    "    \"\"\"\n",
    "    # Remove records where DESC (audit event) != REGULAR\n",
    "    df = df[df.DESC == 'REGULAR']\n",
    "\n",
    "    return df\n",
    "def add_hourly_entries(df):\n",
    "    \"\"\"\n",
    "    The 'ENTRIES' variable recorded in the MTA data are cumulative entries of the turnstile per row. \n",
    "    Considering the data for a single turnstile machine (unique SCP, C/A, and UNIT),\n",
    "    we want to add a new column symbolizing the incremental number of entries since the last recording time.\n",
    "    \n",
    "    This function is to add a new column, calculate the difference between ENTRIES in the current row and the\n",
    "    previous row, and assign the difference to the new column. When there is NaN, fill it with 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    HOURLY_ENTRIES = df.ENTRIES - df.ENTRIES.shift(1) \n",
    "    df['HOURLY_ENTRIES'] = HOURLY_ENTRIES.fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_hourly_exits(df):\n",
    "    \"\"\"\n",
    "    The 'EXITS' variable recorded in the MTA data are cumulative exits of the turnstile per row. \n",
    "    Considering the data for a single turnstile machine (unique SCP, C/A, and UNIT),\n",
    "    we want to add a new column symbolizing the incremental number of exits since the last recording time.\n",
    "    \n",
    "    This function is to add a new column, calculate the difference between EXITS in the current row and the\n",
    "    previous row, and assign the difference to the new column. When there is NaN, fill it with 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    HOURLY_EXITS = df.EXITS - df.EXITS.shift(1) \n",
    "    df['HOURLY_EXITS'] = HOURLY_EXITS.fillna(0)\n",
    "    return df\n",
    "\n",
    "def add_busyness(df):\n",
    "    \"\"\"\n",
    "    Define busyness as sum of ebtries and exits. Add a new column and assign busyness to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    BUSYNESS = df.HOURLY_ENTRIES + df.HOURLY_EXITS \n",
    "    df['BUSYNESS'] = BUSYNESS\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_to_hour(time):\n",
    "    \"\"\"\n",
    "    Input 00:00:00 (hour:minute:second).\n",
    "    Extract and return the hour from input.\n",
    "    \"\"\"\n",
    "    # return pd.to_datetime(time).hour\n",
    "    return int(time.split(':')[0])\n",
    "\n",
    "def date_to_month(date):\n",
    "    \"\"\"\n",
    "    Input mm-dd-yy (month-day-year).\n",
    "    Extract and return the month from input date.\n",
    "    \"\"\"\n",
    "    # return pd.to_datetime(date).month\n",
    "    return int(date.split('-')[0])\n",
    "               \n",
    "def date_to_year(date):\n",
    "    \"\"\"\n",
    "    Input sting mm-dd-yy (month-day-year).\n",
    "    Extract and return the year from input date.\n",
    "    \"\"\"\n",
    "    # return pd.to_datetime(date)\n",
    "    return 2000 + int(date.split('-')[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTA_data(url,start,stop,log_name,date_tag):\n",
    "    \n",
    "    # download data\n",
    "    df = download_data(url,start,stop,log_name)\n",
    "    df.to_csv('MTA_data_{}.csv'.format(date_tag))\n",
    "    \n",
    "    # convert wide form data to long form\n",
    "    df_long=convert_wide_to_long(df)\n",
    "    df_long.to_csv('MTA_data_long_{}.csv'.format(date_tag))\n",
    "    \n",
    "    # filter illegitimate data\n",
    "    df_long = remove_irregular_event(df_long)\n",
    "    df_long.to_csv('MTA_data_regular_{}.csv'.format(date_tag))\n",
    "    \n",
    "    # add hourly incremental entries\n",
    "    df_long = df_long.groupby(['C/A','UNIT','SCP']).apply(\n",
    "        add_hourly_entries)\n",
    "    \n",
    "    # add hourly incremental exits\n",
    "    df_long = df_long.groupby(['C/A','UNIT','SCP']).apply(\n",
    "        add_hourly_exits)\n",
    "\n",
    "    # add a 'HOUR', 'MONTH' and 'YEAR' column \n",
    "    df_long['HOUR'] = df_long['TIME'].map(time_to_hour)\n",
    "    df_long['MONTH'] = df_long['DATE'].map(date_to_month)\n",
    "    df_long['YEAR'] = df_long['DATE'].map(date_to_year)\n",
    "    \n",
    "    df_long.to_csv('MTA_data_hour_{}.csv'.format(date_tag))\n",
    "    \n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://web.mta.info/developers/turnstile.html\"\n",
    "jpy = pq(url)\n",
    "for i in range(2*len(jpy('#contentbox > div > div > a'))):\n",
    "    if (jpy('#contentbox > div > div > a:nth-child({})'.format(i)).text() == \"Saturday, January 04, 2014\"):\n",
    "        stop = i+1\n",
    "        print(\"end_point = \" + str(i))\n",
    "    if (jpy('#contentbox > div > div > a:nth-child({})'.format(i)).text() == \"Saturday, October 11, 2014\"):\n",
    "        start = i\n",
    "        print(\"start_point = \" + str(i))\n",
    "        \n",
    "log_name = 'Download_Log.txt'\n",
    "date_tag = 'y2014' # change here accordingly\n",
    "df_long = MTA_data(url,start,stop,log_name,date_tag)\n",
    "print(df_long.shape)\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    HOURLY_ENTRIES and HOURLY_EXITS contains negative value and abnormally large value (ie. the max is 931476882).\n",
    "    Assuming it takes 1 second for 1 people enter the turnstile, \n",
    "    there can be at max 14,400 people entering turnstile in 4 hours. \n",
    "    So in theory, considering a buffer, any HOURLY_ENTRIES (or HOURLY_EXITS) greater than 20000 is not possible. \n",
    "    Also HOURLY_ENTRIES and HOURLY_EXITS obviously cannot be negative.\n",
    "    \n",
    "    This function replace the negative and greater than 20000 HOURLY_ENTRIES by the mean of the group(ie.SCP,MONTH) that they are in.\n",
    "    Then calculate the \"BUSYNESS\".\n",
    "    \"\"\"\n",
    "\n",
    "    # clean 'HOURLY_ENTRIES'\n",
    "    df['HOURLY_ENTRIES'] = df.groupby(['SCP','MONTH']).HOURLY_ENTRIES.transform(\n",
    "        lambda x: np.where((x<0)|(x>20000),x.mask((x<0)|(x>20000)).mean(),x))\n",
    "    \n",
    "    # clean 'HOURLY_EXITS'\n",
    "    df['HOURLY_EXITS'] = df.groupby(['SCP','MONTH']).HOURLY_EXITS.transform(\n",
    "        lambda x: np.where((x<0)|(x>20000),x.mask((x<0)|(x>20000)).mean(),x))\n",
    "    \n",
    "    # clean 'HOUR' not equal to 0,4,8,12,16,20\n",
    "    \n",
    "    \n",
    "    # add busyness\n",
    "    df['BUSYNESS'] = df.HOURLY_ENTRIES + df.HOURLY_EXITS \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "df_final = clean_data(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mapping from Station_loc_management file\n",
    "from ipynb.fs.full.Station_loc_management import *\n",
    "mapping_final = station_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join the table together\n",
    "df_master = pd.merge(df_final, mapping_final, how='left', on=['C/A','UNIT'], sort=True, copy=True, indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean table\n",
    "df_master['Date_ID'] = (df_master['DATE']).astype('category').cat.codes \n",
    "df_master = df_master.rename(columns = {'Station ID':'Station_ID'})\n",
    "# df_master.to_csv(path_or_buf='./df_master.csv',sep=',', index=None)\n",
    "# df_master = pd.read_csv('./df_master.csv',sep=',')\n",
    "\n",
    "# Standarized hour to 0,4,8,12,16,20\n",
    "def hour_modified(hour):\n",
    "    if hour > 20 or hour <= 0 : return 0\n",
    "    if 0 < hour <= 4 : return 4\n",
    "    if 4 < hour <= 8 : return 8\n",
    "    if 8 < hour <= 12 : return 12\n",
    "    if 12 < hour <= 16 : return 16\n",
    "    if 16 < hour <= 20 : return 20\n",
    "\n",
    "df_master['HOUR_modified'] = df_master['HOUR'].map(hour_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour aggregation based on the date groupby station\n",
    "# Get the simplifed table\n",
    "HOUR_LIST = [0,4,8,12,16,20]\n",
    "col_name = ['Station_ID','HOURLY_ENTRIES','HOURLY_EXITS','Date','HOUR_modified','Station_name','Latitude','Longitude']\n",
    "df_new = pd.DataFrame(columns = col_name)\n",
    "for i in range(len(df_master['Date_ID'].unique())):\n",
    "    \n",
    "    for j in HOUR_LIST:\n",
    "        temp = pd.DataFrame()\n",
    "        temp = df_master[(df_master.Date_ID == i)&(df_master.HOUR_modified == j)].groupby('Station_ID')['HOURLY_ENTRIES','HOURLY_EXITS'].sum().round(2)\n",
    "        temp = temp.reset_index()\n",
    "        temp['Date'] = np.full((len(temp),1),df_master.loc[df_master.Date_ID == i,['DATE']].drop_duplicates().values)\n",
    "        temp['HOUR_modified'] = np.full((len(temp),1),df_master.loc[df_master.HOUR_modified == j,['HOUR_modified']].drop_duplicates().values)\n",
    "        temp = pd.merge(temp, df_master.loc[:,['Station_ID','Station_modified','GTFS Latitude','GTFS Longitude']].drop_duplicates(), how='left', on=['Station_ID']).rename(\n",
    "        columns = {'Station_modified':'Station_name','GTFS Latitude':'Latitude','GTFS Longitude':'Longitude'})\n",
    "        df_new = df_new.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Month and Day\n",
    "def date_to_month(date):\n",
    "    # return pd.to_datetime(date).month\n",
    "    return int(date.split('-')[0])\n",
    "\n",
    "def date_to_day(date):\n",
    "    # return pd.to_datetime(date).month\n",
    "    return int(date.split('-')[1])\n",
    "\n",
    "df_new['Month'] = df_new['Date'].map(date_to_month)\n",
    "df_new['Day'] = df_new['Date'].map(date_to_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(path_or_buf='./final_table_before.csv',sep=',', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append data after 10.11\n",
    "df_new_2 = pd.read_csv('./final_table_after.csv')\n",
    "df_new_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_month_2(date):\n",
    "\n",
    "    # return pd.to_datetime(date).month\n",
    "    return int(date.split('/')[0])\n",
    "\n",
    "def date_to_day_2(date):\n",
    "\n",
    "    # return pd.to_datetime(date).month\n",
    "    return int(date.split('/')[1])\n",
    "\n",
    "df_new_2['Month'] = df_new_2['Date'].map(date_to_month_2)\n",
    "df_new_2['Day'] = df_new_2['Date'].map(date_to_day_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new_final = df_new.append(df_new_2, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly get one day from each month\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "df_new_final_subset = pd.DataFrame()\n",
    "for i in range(1,13):\n",
    "    Date = random.randrange(1, 31, 1)\n",
    "    temp = df_new_final.loc[(df_new_final['Month']==i)&(df_new_final['Day']==Date),:]\n",
    "    df_new_final_subset = df_new_final_subset.append(temp, ignore_index = True)\n",
    "df_new_final_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_final.to_csv(path_or_buf='./final_table_complete.csv',sep=',', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_final_subset.to_csv(path_or_buf='./final_table_complete_subset.csv',sep=',', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
